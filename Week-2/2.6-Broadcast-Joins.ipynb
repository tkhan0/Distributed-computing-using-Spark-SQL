{"cells":[{"cell_type":"markdown","source":["d-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px; height: 163px\">\n</div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4583c44d-9b66-4901-8ac5-f433eefa65f7"}}},{"cell_type":"markdown","source":["# 2.6 Broadcast Joins\n\n## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) In this notebook you:<br>\n* Compare and contrast broadcast and shuffle joins\n* Examine the Physical Plan that is generated for your queries\n* Optimize joins using Broadcast joins"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b4be45b7-b7df-4076-b445-0866d1fe4415"}}},{"cell_type":"code","source":["%run ../Includes/Classroom-Setup"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d2ebd8df-b9e0-4f82-9677-a5cfcac1ba5c"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Broadcast and Shuffle Joins"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7abd9fa8-e581-49ea-a1f5-cd0b960b9c49"}}},{"cell_type":"markdown","source":["-sandbox\n## Standard Join\n\n* In a standard join, **ALL** the data is shuffled.\n* This can be really expensive.\n\n<img src=\"https://files.training.databricks.com/images/eLearning/ucdavis/join-shuffle.png\" alt=\"Join\" style=\"max-height:500px\"/>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cac7e5c0-c650-4adf-9eff-63ecce381d7e"}}},{"cell_type":"markdown","source":["-sandbox\n## Broadcast Join\n* In a Broadcast Join, only the \"small\" data is moved.\n* It duplicates the \"small\" data across all executors.\n* But the \"big\" data is left untouched.\n* If the \"small\" data is small enough, this can be **VERY** efficient.\n\n<img src=\"https://files.training.databricks.com/images/eLearning/ucdavis/join-broadcast.png\" alt=\"Broadcast Join\" style=\"max-height:500px\"/>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"df3e6855-7491-49b7-b5e0-2337626504f7"}}},{"cell_type":"markdown","source":["In brief, joins are _very_ expensive operations.  This becomes increasingly apparent in big data environments where joins involve transferring data across a network."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"62e2b123-b75c-409a-92cd-9e387f0c657e"}}},{"cell_type":"markdown","source":["## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Examining Physical Plans"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e02f885e-8661-454a-8b12-8221d2c52c1a"}}},{"cell_type":"markdown","source":["Let's make sure our data is accessible."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8e146e15-4fbc-4657-bf1d-58782a4654bb"}}},{"cell_type":"code","source":["%sql\nUSE databricks;\n\nDESCRIBE fireCalls"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7e6cf908-26a9-4711-ab76-7a0878ddfa24"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["Call Number","int",null],["Unit ID","string",null],["Incident Number","int",null],["Call Type","string",null],["Call Date","string",null],["Watch Date","string",null],["Received DtTm","string",null],["Entry DtTm","string",null],["Dispatch DtTm","string",null],["Response DtTm","string",null],["On Scene DtTm","string",null],["Transport DtTm","string",null],["Hospital DtTm","string",null],["Call Final Disposition","string",null],["Available DtTm","string",null],["Address","string",null],["City","string",null],["Zipcode of Incident","int",null],["Battalion","string",null],["Station Area","string",null],["Box","string",null],["Original Priority","string",null],["Priority","string",null],["Final Priority","int",null],["ALS Unit","boolean",null],["Call Type Group","string",null],["Number of Alarms","int",null],["Unit Type","string",null],["Unit sequence in call dispatch","int",null],["Fire Prevention District","string",null],["Supervisor District","string",null],["Neighborhooods - Analysis Boundaries","string",null],["Location","string",null],["RowID","string",null]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"col_name","type":"\"string\"","metadata":"{\"comment\":\"name of the column\"}"},{"name":"data_type","type":"\"string\"","metadata":"{\"comment\":\"data type of the column\"}"},{"name":"comment","type":"\"string\"","metadata":"{\"comment\":\"comment of the column\"}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>col_name</th><th>data_type</th><th>comment</th></tr></thead><tbody><tr><td>Call Number</td><td>int</td><td>null</td></tr><tr><td>Unit ID</td><td>string</td><td>null</td></tr><tr><td>Incident Number</td><td>int</td><td>null</td></tr><tr><td>Call Type</td><td>string</td><td>null</td></tr><tr><td>Call Date</td><td>string</td><td>null</td></tr><tr><td>Watch Date</td><td>string</td><td>null</td></tr><tr><td>Received DtTm</td><td>string</td><td>null</td></tr><tr><td>Entry DtTm</td><td>string</td><td>null</td></tr><tr><td>Dispatch DtTm</td><td>string</td><td>null</td></tr><tr><td>Response DtTm</td><td>string</td><td>null</td></tr><tr><td>On Scene DtTm</td><td>string</td><td>null</td></tr><tr><td>Transport DtTm</td><td>string</td><td>null</td></tr><tr><td>Hospital DtTm</td><td>string</td><td>null</td></tr><tr><td>Call Final Disposition</td><td>string</td><td>null</td></tr><tr><td>Available DtTm</td><td>string</td><td>null</td></tr><tr><td>Address</td><td>string</td><td>null</td></tr><tr><td>City</td><td>string</td><td>null</td></tr><tr><td>Zipcode of Incident</td><td>int</td><td>null</td></tr><tr><td>Battalion</td><td>string</td><td>null</td></tr><tr><td>Station Area</td><td>string</td><td>null</td></tr><tr><td>Box</td><td>string</td><td>null</td></tr><tr><td>Original Priority</td><td>string</td><td>null</td></tr><tr><td>Priority</td><td>string</td><td>null</td></tr><tr><td>Final Priority</td><td>int</td><td>null</td></tr><tr><td>ALS Unit</td><td>boolean</td><td>null</td></tr><tr><td>Call Type Group</td><td>string</td><td>null</td></tr><tr><td>Number of Alarms</td><td>int</td><td>null</td></tr><tr><td>Unit Type</td><td>string</td><td>null</td></tr><tr><td>Unit sequence in call dispatch</td><td>int</td><td>null</td></tr><tr><td>Fire Prevention District</td><td>string</td><td>null</td></tr><tr><td>Supervisor District</td><td>string</td><td>null</td></tr><tr><td>Neighborhooods - Analysis Boundaries</td><td>string</td><td>null</td></tr><tr><td>Location</td><td>string</td><td>null</td></tr><tr><td>RowID</td><td>string</td><td>null</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Now create the table `fireCallsParquet`."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cea01036-4fe6-4ba7-9ca1-b917c749fa33"}}},{"cell_type":"code","source":["%sql\nCREATE OR REPLACE TEMPORARY VIEW fireCallsParquet\nUSING Parquet \nOPTIONS (\n    path \"/mnt/davis/fire-calls/fire-calls-8p.parquet\"\n  )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6e92acc2-0196-4ff6-b009-b5dd8f2392fd"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["We can join these two datasets and examine the physical plan (how data is physically affected) using `EXPLAIN`."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bdcf4e35-702e-436c-96f1-4a59f9ce9796"}}},{"cell_type":"code","source":["%sql\nEXPLAIN \n  SELECT * \n  FROM fireCalls \n  JOIN fireCallsParquet on fireCalls.`Call Number` = fireCallsParquet.`Call_Number`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"736ed5fe-0ee8-4955-8df3-b5d3de73b87e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["== Physical Plan ==\n*(3) SortMergeJoin [Call Number#369], [Call_Number#298], Inner\n:- Sort [Call Number#369 ASC NULLS FIRST], false, 0\n:  +- Exchange hashpartitioning(Call Number#369, 200)\n:     +- *(1) Project [Call Number#369, Unit ID#370, Incident Number#371, Call Type#372, Call Date#373, Watch Date#374, Received DtTm#375, Entry DtTm#376, Dispatch DtTm#377, Response DtTm#378, On Scene DtTm#379, Transport DtTm#380, Hospital DtTm#381, Call Final Disposition#382, Available DtTm#383, Address#384, City#385, Zipcode of Incident#386, Battalion#387, Station Area#388, Box#389, Original Priority#390, Priority#391, Final Priority#392, ... 10 more fields]\n:        +- *(1) Filter isnotnull(Call Number#369)\n:           +- *(1) FileScan csv databricks.firecalls[Call Number#369,Unit ID#370,Incident Number#371,Call Type#372,Call Date#373,Watch Date#374,Received DtTm#375,Entry DtTm#376,Dispatch DtTm#377,Response DtTm#378,On Scene DtTm#379,Transport DtTm#380,Hospital DtTm#381,Call Final Disposition#382,Available DtTm#383,Address#384,City#385,Zipcode of Incident#386,Battalion#387,Station Area#388,Box#389,Original Priority#390,Priority#391,Final Priority#392,... 10 more fields] Batched: false, DataFilters: [isnotnull(Call Number#369)], Format: CSV, Location: InMemoryFileIndex[dbfs:/mnt/davis/fire-calls/fire-calls-truncated-comma.csv], PartitionFilters: [], PushedFilters: [IsNotNull(Call Number)], ReadSchema: struct<Call Number:int,Unit ID:string,Incident Number:int,Call Type:string,Call Date:string,Watch...\n+- Sort [Call_Number#298 ASC NULLS FIRST], false, 0\n   +- Exchange hashpartitioning(Call_Number#298, 200)\n      +- *(2) Project [Call_Number#298, Unit_ID#299, Incident_Number#300, Call_Type#301, Call_Date#302, Watch_Date#303, Received_DtTm#304, Entry_DtTm#305, Dispatch_DtTm#306, Response_DtTm#307, On_Scene_DtTm#308, Transport_DtTm#309, Hospital_DtTm#310, Call_Final_Disposition#311, Available_DtTm#312, Address#313, City#314, Zipcode_of_Incident#315, Battalion#316, Station_Area#317, Box#318, Original_Priority#319, Priority#320, Final_Priority#321, ... 10 more fields]\n         +- *(2) Filter isnotnull(Call_Number#298)\n            +- *(2) FileScan parquet [Call_Number#298,Unit_ID#299,Incident_Number#300,Call_Type#301,Call_Date#302,Watch_Date#303,Received_DtTm#304,Entry_DtTm#305,Dispatch_DtTm#306,Response_DtTm#307,On_Scene_DtTm#308,Transport_DtTm#309,Hospital_DtTm#310,Call_Final_Disposition#311,Available_DtTm#312,Address#313,City#314,Zipcode_of_Incident#315,Battalion#316,Station_Area#317,Box#318,Original_Priority#319,Priority#320,Final_Priority#321,... 10 more fields] Batched: true, DataFilters: [isnotnull(Call_Number#298)], Format: Parquet, Location: InMemoryFileIndex[dbfs:/mnt/davis/fire-calls/fire-calls-8p.parquet], PartitionFilters: [], PushedFilters: [IsNotNull(Call_Number)], ReadSchema: struct<Call_Number:int,Unit_ID:string,Incident_Number:int,Call_Type:string,Call_Date:string,Watch..."]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"plan","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>plan</th></tr></thead><tbody><tr><td>== Physical Plan ==\n*(3) SortMergeJoin [Call Number#369], [Call_Number#298], Inner\n:- Sort [Call Number#369 ASC NULLS FIRST], false, 0\n:  +- Exchange hashpartitioning(Call Number#369, 200)\n:     +- *(1) Project [Call Number#369, Unit ID#370, Incident Number#371, Call Type#372, Call Date#373, Watch Date#374, Received DtTm#375, Entry DtTm#376, Dispatch DtTm#377, Response DtTm#378, On Scene DtTm#379, Transport DtTm#380, Hospital DtTm#381, Call Final Disposition#382, Available DtTm#383, Address#384, City#385, Zipcode of Incident#386, Battalion#387, Station Area#388, Box#389, Original Priority#390, Priority#391, Final Priority#392, ... 10 more fields]\n:        +- *(1) Filter isnotnull(Call Number#369)\n:           +- *(1) FileScan csv databricks.firecalls[Call Number#369,Unit ID#370,Incident Number#371,Call Type#372,Call Date#373,Watch Date#374,Received DtTm#375,Entry DtTm#376,Dispatch DtTm#377,Response DtTm#378,On Scene DtTm#379,Transport DtTm#380,Hospital DtTm#381,Call Final Disposition#382,Available DtTm#383,Address#384,City#385,Zipcode of Incident#386,Battalion#387,Station Area#388,Box#389,Original Priority#390,Priority#391,Final Priority#392,... 10 more fields] Batched: false, DataFilters: [isnotnull(Call Number#369)], Format: CSV, Location: InMemoryFileIndex[dbfs:/mnt/davis/fire-calls/fire-calls-truncated-comma.csv], PartitionFilters: [], PushedFilters: [IsNotNull(Call Number)], ReadSchema: struct<Call Number:int,Unit ID:string,Incident Number:int,Call Type:string,Call Date:string,Watch...\n+- Sort [Call_Number#298 ASC NULLS FIRST], false, 0\n   +- Exchange hashpartitioning(Call_Number#298, 200)\n      +- *(2) Project [Call_Number#298, Unit_ID#299, Incident_Number#300, Call_Type#301, Call_Date#302, Watch_Date#303, Received_DtTm#304, Entry_DtTm#305, Dispatch_DtTm#306, Response_DtTm#307, On_Scene_DtTm#308, Transport_DtTm#309, Hospital_DtTm#310, Call_Final_Disposition#311, Available_DtTm#312, Address#313, City#314, Zipcode_of_Incident#315, Battalion#316, Station_Area#317, Box#318, Original_Priority#319, Priority#320, Final_Priority#321, ... 10 more fields]\n         +- *(2) Filter isnotnull(Call_Number#298)\n            +- *(2) FileScan parquet [Call_Number#298,Unit_ID#299,Incident_Number#300,Call_Type#301,Call_Date#302,Watch_Date#303,Received_DtTm#304,Entry_DtTm#305,Dispatch_DtTm#306,Response_DtTm#307,On_Scene_DtTm#308,Transport_DtTm#309,Hospital_DtTm#310,Call_Final_Disposition#311,Available_DtTm#312,Address#313,City#314,Zipcode_of_Incident#315,Battalion#316,Station_Area#317,Box#318,Original_Priority#319,Priority#320,Final_Priority#321,... 10 more fields] Batched: true, DataFilters: [isnotnull(Call_Number#298)], Format: Parquet, Location: InMemoryFileIndex[dbfs:/mnt/davis/fire-calls/fire-calls-8p.parquet], PartitionFilters: [], PushedFilters: [IsNotNull(Call_Number)], ReadSchema: struct<Call_Number:int,Unit_ID:string,Incident_Number:int,Call_Type:string,Call_Date:string,Watch...</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### Automatic and Manual broadcasting\n\n- Depending on size of the data that is being loaded into Spark, Spark uses internal heuristics to decide how to join that data to other data.\n- Automatic broadcast depends on `spark.sql.autoBroadcastJoinThreshold`\n    - The setting configures the **maximum size in bytes** for a table that will be broadcast to all worker nodes when performing a join \n    - Default is 10MB\n\n- A `broadcast` function can be used in Spark to instruct Catalyst that it should probably broadcast one of the tables that is being joined. \n\nIf the `broadcast` hint isn't used, but one side of the join is small enough (i.e., its size is below the threshold), that data source will be read into\nthe Driver and broadcast to all Executors."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8d557068-dd95-4200-9a34-25f5f231f5f6"}}},{"cell_type":"code","source":["%python\nspark.conf.get(\"spark.sql.autoBroadcastJoinThreshold\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a27ccdb7-3dd0-4e37-9a75-c1bf78f02c0b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[1]: &#39;10485760&#39;</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[1]: &#39;10485760&#39;</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Now take a look at the physical plan when we broadcast one of the datasets.  The broadcast join hint is going to operate like a SQL hint, but Spark will still parse this even though it is commented out."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"159f9dd1-c5bb-4baf-9ef8-3e39e1fd9e03"}}},{"cell_type":"code","source":["%sql\nEXPLAIN \n  SELECT /*+ BROADCAST(fireCalls) */ * \n  FROM fireCalls \n  JOIN fireCallsParquet on fireCalls.`Call Number` = fireCallsParquet.`Call_Number`\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3782713b-9d03-4bc4-be1d-cc0f48f6faba"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["== Physical Plan ==\n*(2) BroadcastHashJoin [Call Number#369], [Call_Number#298], Inner, BuildLeft\n:- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n:  +- *(1) Project [Call Number#369, Unit ID#370, Incident Number#371, Call Type#372, Call Date#373, Watch Date#374, Received DtTm#375, Entry DtTm#376, Dispatch DtTm#377, Response DtTm#378, On Scene DtTm#379, Transport DtTm#380, Hospital DtTm#381, Call Final Disposition#382, Available DtTm#383, Address#384, City#385, Zipcode of Incident#386, Battalion#387, Station Area#388, Box#389, Original Priority#390, Priority#391, Final Priority#392, ... 10 more fields]\n:     +- *(1) Filter isnotnull(Call Number#369)\n:        +- *(1) FileScan csv databricks.firecalls[Call Number#369,Unit ID#370,Incident Number#371,Call Type#372,Call Date#373,Watch Date#374,Received DtTm#375,Entry DtTm#376,Dispatch DtTm#377,Response DtTm#378,On Scene DtTm#379,Transport DtTm#380,Hospital DtTm#381,Call Final Disposition#382,Available DtTm#383,Address#384,City#385,Zipcode of Incident#386,Battalion#387,Station Area#388,Box#389,Original Priority#390,Priority#391,Final Priority#392,... 10 more fields] Batched: false, DataFilters: [isnotnull(Call Number#369)], Format: CSV, Location: InMemoryFileIndex[dbfs:/mnt/davis/fire-calls/fire-calls-truncated-comma.csv], PartitionFilters: [], PushedFilters: [IsNotNull(Call Number)], ReadSchema: struct<Call Number:int,Unit ID:string,Incident Number:int,Call Type:string,Call Date:string,Watch...\n+- *(2) Project [Call_Number#298, Unit_ID#299, Incident_Number#300, Call_Type#301, Call_Date#302, Watch_Date#303, Received_DtTm#304, Entry_DtTm#305, Dispatch_DtTm#306, Response_DtTm#307, On_Scene_DtTm#308, Transport_DtTm#309, Hospital_DtTm#310, Call_Final_Disposition#311, Available_DtTm#312, Address#313, City#314, Zipcode_of_Incident#315, Battalion#316, Station_Area#317, Box#318, Original_Priority#319, Priority#320, Final_Priority#321, ... 10 more fields]\n   +- *(2) Filter isnotnull(Call_Number#298)\n      +- *(2) FileScan parquet [Call_Number#298,Unit_ID#299,Incident_Number#300,Call_Type#301,Call_Date#302,Watch_Date#303,Received_DtTm#304,Entry_DtTm#305,Dispatch_DtTm#306,Response_DtTm#307,On_Scene_DtTm#308,Transport_DtTm#309,Hospital_DtTm#310,Call_Final_Disposition#311,Available_DtTm#312,Address#313,City#314,Zipcode_of_Incident#315,Battalion#316,Station_Area#317,Box#318,Original_Priority#319,Priority#320,Final_Priority#321,... 10 more fields] Batched: true, DataFilters: [isnotnull(Call_Number#298)], Format: Parquet, Location: InMemoryFileIndex[dbfs:/mnt/davis/fire-calls/fire-calls-8p.parquet], PartitionFilters: [], PushedFilters: [IsNotNull(Call_Number)], ReadSchema: struct<Call_Number:int,Unit_ID:string,Incident_Number:int,Call_Type:string,Call_Date:string,Watch..."]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"plan","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>plan</th></tr></thead><tbody><tr><td>== Physical Plan ==\n*(2) BroadcastHashJoin [Call Number#369], [Call_Number#298], Inner, BuildLeft\n:- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n:  +- *(1) Project [Call Number#369, Unit ID#370, Incident Number#371, Call Type#372, Call Date#373, Watch Date#374, Received DtTm#375, Entry DtTm#376, Dispatch DtTm#377, Response DtTm#378, On Scene DtTm#379, Transport DtTm#380, Hospital DtTm#381, Call Final Disposition#382, Available DtTm#383, Address#384, City#385, Zipcode of Incident#386, Battalion#387, Station Area#388, Box#389, Original Priority#390, Priority#391, Final Priority#392, ... 10 more fields]\n:     +- *(1) Filter isnotnull(Call Number#369)\n:        +- *(1) FileScan csv databricks.firecalls[Call Number#369,Unit ID#370,Incident Number#371,Call Type#372,Call Date#373,Watch Date#374,Received DtTm#375,Entry DtTm#376,Dispatch DtTm#377,Response DtTm#378,On Scene DtTm#379,Transport DtTm#380,Hospital DtTm#381,Call Final Disposition#382,Available DtTm#383,Address#384,City#385,Zipcode of Incident#386,Battalion#387,Station Area#388,Box#389,Original Priority#390,Priority#391,Final Priority#392,... 10 more fields] Batched: false, DataFilters: [isnotnull(Call Number#369)], Format: CSV, Location: InMemoryFileIndex[dbfs:/mnt/davis/fire-calls/fire-calls-truncated-comma.csv], PartitionFilters: [], PushedFilters: [IsNotNull(Call Number)], ReadSchema: struct<Call Number:int,Unit ID:string,Incident Number:int,Call Type:string,Call Date:string,Watch...\n+- *(2) Project [Call_Number#298, Unit_ID#299, Incident_Number#300, Call_Type#301, Call_Date#302, Watch_Date#303, Received_DtTm#304, Entry_DtTm#305, Dispatch_DtTm#306, Response_DtTm#307, On_Scene_DtTm#308, Transport_DtTm#309, Hospital_DtTm#310, Call_Final_Disposition#311, Available_DtTm#312, Address#313, City#314, Zipcode_of_Incident#315, Battalion#316, Station_Area#317, Box#318, Original_Priority#319, Priority#320, Final_Priority#321, ... 10 more fields]\n   +- *(2) Filter isnotnull(Call_Number#298)\n      +- *(2) FileScan parquet [Call_Number#298,Unit_ID#299,Incident_Number#300,Call_Type#301,Call_Date#302,Watch_Date#303,Received_DtTm#304,Entry_DtTm#305,Dispatch_DtTm#306,Response_DtTm#307,On_Scene_DtTm#308,Transport_DtTm#309,Hospital_DtTm#310,Call_Final_Disposition#311,Available_DtTm#312,Address#313,City#314,Zipcode_of_Incident#315,Battalion#316,Station_Area#317,Box#318,Original_Priority#319,Priority#320,Final_Priority#321,... 10 more fields] Batched: true, DataFilters: [isnotnull(Call_Number#298)], Format: Parquet, Location: InMemoryFileIndex[dbfs:/mnt/davis/fire-calls/fire-calls-8p.parquet], PartitionFilters: [], PushedFilters: [IsNotNull(Call_Number)], ReadSchema: struct<Call_Number:int,Unit_ID:string,Incident_Number:int,Call_Type:string,Call_Date:string,Watch...</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\n&copy; 2020 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"025600b1-357a-4361-a44d-27ad6d1302f4"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"2.6-Broadcast-Joins","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":1320160929412258}},"nbformat":4,"nbformat_minor":0}
